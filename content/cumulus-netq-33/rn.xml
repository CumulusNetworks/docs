<tables>
<table name="Open Issues in 3.3.1">
<tr>
<th> Issue ID </th>
<th> Description </th>
<th> Affects </th>
<th> Fixed </th>
</tr>
<tr>
<td>2555854</td>
<td>If a NetQ Agent is downgraded to the 3.0.0 version from any higher release, the default commands file present in the _/etc/netq/commands/_ also needs to be updated to prevent the NetQ Agent from becoming rotten.</td>
<td>3.0.0-3.3.1</td>
<td></td>
</tr>
<tr>
<td>2555197</td>
<td>NetQ CLI: Occasionally, when a command response contains a large number of objects to be displayed the NetQ CLI does not display all results in the console. When this occurs, view all results using the {{json}} format option.</td>
<td>3.3.0-3.3.1</td>
<td></td>
</tr>
<tr>
<td>2553453</td>
<td>The {{netqd}} daemon logs a traceback to _/var/log/netqd.log_ when the OPTA server is unreachable and {{netq show}} commands are run.</td>
<td>3.1.0-3.3.1</td>
<td></td>
</tr>
<tr>
<td>2551545</td>
<td>Infra: Rarely, after a node is restarted, Kubernetes pods do not synchronize properly and the output of {{netq show opta-health}} shows failures. Node operation is not functionally impacted. You can safely remove the failures by running {{kubectl get pods | grep MatchNodeSelector | cut \-f1 \-d' ' | xargs kubectl delete pod}}. To work around the issue, do not label nodes using the API. Instead label nodes through local configuration using {{kubelet flag "--node-labels"}}.</td>
<td>3.1.0-3.3.1</td>
<td></td>
</tr>
<tr>
<td>2549649</td>
<td>UI: Warnings might appear during the post-upgrade phase for a Cumulus Linux switch upgrade job. They are caused by services that have not yet been restored by the time the job is complete. Cumulus Networks recommend waiting five minutes, creating a network snapshot, then comparing that to the pre-upgrade snapshot. If the comparison shows no differences for the services, the warnings can be ignored. If there are differences, then troubleshooting the relevant service(s) is recommended.</td>
<td>3.0.0-3.3.1</td>
<td></td>
</tr>
<tr>
<td>2549319</td>
<td>UI: The legend and segment colors on Switches and Upgrade History card graphs sometimes do not match. These cards appear on the lifecycle management dashboard (Manage Switch Assets view). Hover over graph to view the correct values.</td>
<td>3.0.0-3.3.1</td>
<td></td>
</tr>
<tr>
<td>2543867</td>
<td>UI: If either the peer_hostname or the peer_asn is invalid, the full screen BGP Service card does not provide the ability to open cards for a selected BGP session.</td>
<td>2.3.0-2.4.1, 3.0.0-3.3.1</td>
<td></td>
</tr>
</table>
<table name="Fixed issues in 3.3.1">
<tr>
<th> Issue ID </th>
<th> Description </th>
<th> Affects </th>
</tr>
<tr>
<td>2556545</td>
<td>NetQ Agent: When upgrading to NetQ 3.3.0, sometimes the NetQ Agent fails to complete the upgrade on Broadcom-based swtiches. To work around this issue, update the following file on the switch where the NetQ Agent is running:
1. Open the _armel-netq-commands.yml_ file in the _/etc/netq/commands/_ directory for editing.
2. Copy and append the following to the file:
{code} 
- service: "inventory"
    commands:
      - key: "os-release"
        command: "cat /etc/os-release"
        isactive: true
        parser: "local"
      - key: "eprom"
        command: "/usr/cumulus/bin/decode-syseeprom -j"
        isactive: true
        parser: "local"
      - key: "lscpu"
        command: "/usr/bin/lscpu"
        isactive: true
        parser: "local"
      - key: "meminfo"
        command: "cat /proc/meminfo"
        isactive: true
        parser: "local"
      - key: "lsblk"
        command: "lsblk -d -n -o name,size,type,vendor,tran,rev,model"
        isactive: true
        parser: "local"
      - key: "dmicode"
        command: "dmidecode -t 17"
        isactive: true
        parser: "local"
      - key: "is-opta"
        command: "cat /etc/app-release"
        isactive: true
        parser: "local"
{code}
3. Remove the existing generated command file.
{code}
cumulus@switch:~$ rm /var/run/netq/netq-commands.yml
{code}
4. Restart the NetQ Agent.
{code}
cumulus@switch:~$ netq config restart agent
{code}</td>
<td>3.3.0</td>
</tr>
<tr>
<td>2556434</td>
<td>NetQ UI: When switches are running NetQ but hosts are not, the topology does not display the spine and leaf layers as separate tiers.</td>
<td>3.3.0</td>
</tr>
<tr>
<td>2556275</td>
<td>NetQ UI: The full-screen BGP Validation card for the default validation can become unresponsive when a very large number of sessions or errors are present.</td>
<td>3.3.0</td>
</tr>
<tr>
<td>2556268</td>
<td>NetQ UI: When assigning a switch configuration profile to a switch using the lifecycle management, you cannot save the per-switch variable data. This prevents you from applying the switch configuration. Upgrade to NetQ 3.3.1 to take advantage of this feature.</td>
<td>3.3.0</td>
</tr>
<tr>
<td>2556227</td>
<td>Admin UI: For cloud deployments, clicking the _Open NetQ_ link at the bottom of the Admin UI NetQ Health page returns an error _default backend - 404_ as it attempts to open the NetQ UI on the on-site NetQ Cloud Appliance or VM running the NetQ Collector software. In cloud deployments, the NetQ UI is run in the Cloud rather than locally, thus causing the error. To open the NetQ UI and view your data, enter [https://netq.cumulusnetworks.com/] into the address bar of your browser.</td>
<td>3.3.0</td>
</tr>
<tr>
<td>2556205</td>
<td>NetQ CLI: User cannot remove a notification channel when threshold-based event rules are configured..</td>
<td>3.2.1-3.3.0</td>
</tr>
<tr>
<td>2556192</td>
<td>NetQ UI: In multi-site on-premises deployments, when a new premises (created using the Premises card) is selected from the dropdown in the application header, the NetQ UI becomes unresponsive for many navigation tabs. To work around this issue, manually update the database as follows:
# In a terminal window, open the database shell (cqlsh).
{code}
$ CASSANDRA_POD=&apos;kubectl get pods | grep cassandra | cut -f1 -d" "&apos; ; kubectl exec -it $CASSANDRA_POD -- cqlsh
{code}
# Display the premises table.
{code}
cqlsh> select opid,name,namespace from master.premises ;
{code}
The resulting output would be similar to this, with new premises having an empty namespace:
{code}
 opid  | name  | namespace
-------+-------+-----------
 20001 | site1 |      null
     0 | OPID0 |      null
 20002 | site2 |          
 20000 | site0 |      null
 20003 | site3 |          
(5 rows)
{code}
# For each new premises, insert a _null_ value into the database to resolve the issue. For example the new premises _site3_ has an empty namespace value, run following query against its opid _20003_ to change the value.
{code}
cqlsh> insert into master.premises (opid,namespace) values (20003,null);
{code}
# Verify the new premises now have null values in the database.
{code}
cqlsh> select opid,name,namespace from master.premises ;
 opid  | name  | namespace
-------+-------+-----------
 20001 | site1 |      null
     0 | OPID0 |      null
 20002 | site2 |      null     
 20000 | site0 |      null
 20003 | site3 |      null
(5 rows)
{code}</td>
<td>3.3.0</td>
</tr>
<tr>
<td>2556117</td>
<td>NetQ Infra: The NetQ Agent fails to start when switch is running Cumulus Linux version 4.1.1 or 4.2.0. To work around this issue, run the following on each switch:
For CL 4.1.1, rename the WJH types file to the 4.3.3260 version:
{code:java}
cumulus@switch:~$ sudo mv /usr/lib/cumulus/wjh/wjh_types_4.4.3260.py /usr/lib/cumulus/wjh/wjh_types_4.3.3260.py
cumulus@switch:~$ netq config restart agent
{code}
For CL 4.2.0, edit the following line in the /usr/sbin/netq-agent-prestart script to change the version from 4.4.095 to 4.4.0952:
{code:java}
elif [ $sx_sdk_ver == “4.4.0952” ] || [  $sx_sdk_ver == “4.4.1624” ] || [ $sx_sdk_ver == “4.3.3260” ]; then
cumulus@switch:~$ netq config restart agent
{code}</td>
<td>3.3.0</td>
</tr>
<tr>
<td>2556007</td>
<td>NetQ API: Several APIs are presenting the following error when viewed in Swagger UI:
{code}
Fetch errorundefined https://api.prod2.netq.cumulusnetworks.com/netq/telemetry/v1/api-docs/events/swagger.json
{code}
To correct this presentation issue:
# Open the netqui YAML file for editing.
{code}
kubectl edit netquis netqui
{code}
# Locate the _misc_ section. For example:
{code}
misc:
  cassandraReconnectLogOnly: "true"
  clusterName: netq
  forgotPasswordLink: /link/to/set/password
  ...
  smtpSSL: "true"
  tlsEnabled: "true"
{code}
# Add the {{document_namespace}} parameter below the {{tlsEnabled}} parameter.
{code}
misc:
  cassandraReconnectLogOnly: "true"
  clusterName: netq
  forgotPasswordLink: /link/to/set/password
  ...
  smtpSSL: "true"
  tlsEnabled: "true"
  document_namespace: "default"
{code}
# Save the file.</td>
<td>3.3.0</td>
</tr>
<tr>
<td>2555848</td>
<td>NetQ Infra: It is important to plan your upgrade to NetQ 3.3.0 because the NetQ Appliance or VM becomes unavailable for approximately an hour during the process. No data is lost in the process.</td>
<td>3.3.0</td>
</tr>
<tr>
<td>2555617</td>
<td>NetQ Infra: Upgrading the NetQ Agent before upgrading the NetQ CLI for version 3.3.0 causes the NetQ CLI to fail the upgrade. To work around this issue, upgrade the NetQ CLI first, then follow with the NetQ Agent upgrade.</td>
<td>3.3.0</td>
</tr>
<tr>
<td>2555587</td>
<td>NetQ UI: Switches with LLDP enabled only on eth0 are not shown on the topology diagram.</td>
<td>3.3.0</td>
</tr>
</table>
<table name="Open Issues in 3.3.0">
<tr>
<th> Issue ID </th>
<th> Description </th>
<th> Affects </th>
<th> Fixed </th>
</tr>
<tr>
<td>2556545</td>
<td>NetQ Agent: When upgrading to NetQ 3.3.0, sometimes the NetQ Agent fails to complete the upgrade on Broadcom-based swtiches. To work around this issue, update the following file on the switch where the NetQ Agent is running:
1. Open the _armel-netq-commands.yml_ file in the _/etc/netq/commands/_ directory for editing.
2. Copy and append the following to the file:
{code} 
- service: "inventory"
    commands:
      - key: "os-release"
        command: "cat /etc/os-release"
        isactive: true
        parser: "local"
      - key: "eprom"
        command: "/usr/cumulus/bin/decode-syseeprom -j"
        isactive: true
        parser: "local"
      - key: "lscpu"
        command: "/usr/bin/lscpu"
        isactive: true
        parser: "local"
      - key: "meminfo"
        command: "cat /proc/meminfo"
        isactive: true
        parser: "local"
      - key: "lsblk"
        command: "lsblk -d -n -o name,size,type,vendor,tran,rev,model"
        isactive: true
        parser: "local"
      - key: "dmicode"
        command: "dmidecode -t 17"
        isactive: true
        parser: "local"
      - key: "is-opta"
        command: "cat /etc/app-release"
        isactive: true
        parser: "local"
{code}
3. Remove the existing generated command file.
{code}
cumulus@switch:~$ rm /var/run/netq/netq-commands.yml
{code}
4. Restart the NetQ Agent.
{code}
cumulus@switch:~$ netq config restart agent
{code}</td>
<td>3.3.0</td>
<td>3.3.1</td>
</tr>
<tr>
<td>2556434</td>
<td>NetQ UI: When switches are running NetQ but hosts are not, the topology does not display the spine and leaf layers as separate tiers.</td>
<td>3.3.0</td>
<td>3.3.1</td>
</tr>
<tr>
<td>2556275</td>
<td>NetQ UI: The full-screen BGP Validation card for the default validation can become unresponsive when a very large number of sessions or errors are present.</td>
<td>3.3.0</td>
<td>3.3.1</td>
</tr>
<tr>
<td>2556268</td>
<td>NetQ UI: When assigning a switch configuration profile to a switch using the lifecycle management, you cannot save the per-switch variable data. This prevents you from applying the switch configuration. Upgrade to NetQ 3.3.1 to take advantage of this feature.</td>
<td>3.3.0</td>
<td>3.3.1</td>
</tr>
<tr>
<td>2556227</td>
<td>Admin UI: For cloud deployments, clicking the _Open NetQ_ link at the bottom of the Admin UI NetQ Health page returns an error _default backend - 404_ as it attempts to open the NetQ UI on the on-site NetQ Cloud Appliance or VM running the NetQ Collector software. In cloud deployments, the NetQ UI is run in the Cloud rather than locally, thus causing the error. To open the NetQ UI and view your data, enter [https://netq.cumulusnetworks.com/] into the address bar of your browser.</td>
<td>3.3.0</td>
<td>3.3.1</td>
</tr>
<tr>
<td>2556205</td>
<td>NetQ CLI: User cannot remove a notification channel when threshold-based event rules are configured..</td>
<td>3.2.1-3.3.0</td>
<td>3.3.1</td>
</tr>
<tr>
<td>2556192</td>
<td>NetQ UI: In multi-site on-premises deployments, when a new premises (created using the Premises card) is selected from the dropdown in the application header, the NetQ UI becomes unresponsive for many navigation tabs. To work around this issue, manually update the database as follows:
# In a terminal window, open the database shell (cqlsh).
{code}
$ CASSANDRA_POD=&apos;kubectl get pods | grep cassandra | cut -f1 -d" "&apos; ; kubectl exec -it $CASSANDRA_POD -- cqlsh
{code}
# Display the premises table.
{code}
cqlsh> select opid,name,namespace from master.premises ;
{code}
The resulting output would be similar to this, with new premises having an empty namespace:
{code}
 opid  | name  | namespace
-------+-------+-----------
 20001 | site1 |      null
     0 | OPID0 |      null
 20002 | site2 |          
 20000 | site0 |      null
 20003 | site3 |          
(5 rows)
{code}
# For each new premises, insert a _null_ value into the database to resolve the issue. For example the new premises _site3_ has an empty namespace value, run following query against its opid _20003_ to change the value.
{code}
cqlsh> insert into master.premises (opid,namespace) values (20003,null);
{code}
# Verify the new premises now have null values in the database.
{code}
cqlsh> select opid,name,namespace from master.premises ;
 opid  | name  | namespace
-------+-------+-----------
 20001 | site1 |      null
     0 | OPID0 |      null
 20002 | site2 |      null     
 20000 | site0 |      null
 20003 | site3 |      null
(5 rows)
{code}</td>
<td>3.3.0</td>
<td>3.3.1</td>
</tr>
<tr>
<td>2556117</td>
<td>NetQ Infra: The NetQ Agent fails to start when switch is running Cumulus Linux version 4.1.1 or 4.2.0. To work around this issue, run the following on each switch:
For CL 4.1.1, rename the WJH types file to the 4.3.3260 version:
{code:java}
cumulus@switch:~$ sudo mv /usr/lib/cumulus/wjh/wjh_types_4.4.3260.py /usr/lib/cumulus/wjh/wjh_types_4.3.3260.py
cumulus@switch:~$ netq config restart agent
{code}
For CL 4.2.0, edit the following line in the /usr/sbin/netq-agent-prestart script to change the version from 4.4.095 to 4.4.0952:
{code:java}
elif [ $sx_sdk_ver == “4.4.0952” ] || [  $sx_sdk_ver == “4.4.1624” ] || [ $sx_sdk_ver == “4.3.3260” ]; then
cumulus@switch:~$ netq config restart agent
{code}</td>
<td>3.3.0</td>
<td>3.3.1</td>
</tr>
<tr>
<td>2556015</td>
<td>NetQ Infra: When adding a data source with the Grafana plugin, an error message appears "Cannot read property 'status' of undefined" implying the data source is not added when, in fact, it has been. Ignore the error and continue with configuring the plugin.</td>
<td>3.3.0</td>
<td>3.3.1</td>
</tr>
<tr>
<td>2556007</td>
<td>NetQ API: Several APIs are presenting the following error when viewed in Swagger UI:
{code}
Fetch errorundefined https://api.prod2.netq.cumulusnetworks.com/netq/telemetry/v1/api-docs/events/swagger.json
{code}
To correct this presentation issue:
# Open the netqui YAML file for editing.
{code}
kubectl edit netquis netqui
{code}
# Locate the _misc_ section. For example:
{code}
misc:
  cassandraReconnectLogOnly: "true"
  clusterName: netq
  forgotPasswordLink: /link/to/set/password
  ...
  smtpSSL: "true"
  tlsEnabled: "true"
{code}
# Add the {{document_namespace}} parameter below the {{tlsEnabled}} parameter.
{code}
misc:
  cassandraReconnectLogOnly: "true"
  clusterName: netq
  forgotPasswordLink: /link/to/set/password
  ...
  smtpSSL: "true"
  tlsEnabled: "true"
  document_namespace: "default"
{code}
# Save the file.</td>
<td>3.3.0</td>
<td>3.3.1</td>
</tr>
<tr>
<td>2555854</td>
<td>If a NetQ Agent is downgraded to the 3.0.0 version from any higher release, the default commands file present in the _/etc/netq/commands/_ also needs to be updated to prevent the NetQ Agent from becoming rotten.</td>
<td>3.0.0-3.3.1</td>
<td></td>
</tr>
<tr>
<td>2555848</td>
<td>NetQ Infra: It is important to plan your upgrade to NetQ 3.3.0 because the NetQ Appliance or VM becomes unavailable for approximately an hour during the process. No data is lost in the process.</td>
<td>3.3.0</td>
<td>3.3.1</td>
</tr>
<tr>
<td>2555617</td>
<td>NetQ Infra: Upgrading the NetQ Agent before upgrading the NetQ CLI for version 3.3.0 causes the NetQ CLI to fail the upgrade. To work around this issue, upgrade the NetQ CLI first, then follow with the NetQ Agent upgrade.</td>
<td>3.3.0</td>
<td>3.3.1</td>
</tr>
<tr>
<td>2555587</td>
<td>NetQ UI: Switches with LLDP enabled only on eth0 are not shown on the topology diagram.</td>
<td>3.3.0</td>
<td>3.3.1</td>
</tr>
<tr>
<td>2555197</td>
<td>NetQ CLI: Occasionally, when a command response contains a large number of objects to be displayed the NetQ CLI does not display all results in the console. When this occurs, view all results using the {{json}} format option.</td>
<td>3.3.0-3.3.1</td>
<td></td>
</tr>
<tr>
<td>2553453</td>
<td>The {{netqd}} daemon logs a traceback to _/var/log/netqd.log_ when the OPTA server is unreachable and {{netq show}} commands are run.</td>
<td>3.1.0-3.3.1</td>
<td></td>
</tr>
<tr>
<td>2551545</td>
<td>Infra: Rarely, after a node is restarted, Kubernetes pods do not synchronize properly and the output of {{netq show opta-health}} shows failures. Node operation is not functionally impacted. You can safely remove the failures by running {{kubectl get pods | grep MatchNodeSelector | cut \-f1 \-d' ' | xargs kubectl delete pod}}. To work around the issue, do not label nodes using the API. Instead label nodes through local configuration using {{kubelet flag "--node-labels"}}.</td>
<td>3.1.0-3.3.1</td>
<td></td>
</tr>
<tr>
<td>2549649</td>
<td>UI: Warnings might appear during the post-upgrade phase for a Cumulus Linux switch upgrade job. They are caused by services that have not yet been restored by the time the job is complete. Cumulus Networks recommend waiting five minutes, creating a network snapshot, then comparing that to the pre-upgrade snapshot. If the comparison shows no differences for the services, the warnings can be ignored. If there are differences, then troubleshooting the relevant service(s) is recommended.</td>
<td>3.0.0-3.3.1</td>
<td></td>
</tr>
<tr>
<td>2549319</td>
<td>UI: The legend and segment colors on Switches and Upgrade History card graphs sometimes do not match. These cards appear on the lifecycle management dashboard (Manage Switch Assets view). Hover over graph to view the correct values.</td>
<td>3.0.0-3.3.1</td>
<td></td>
</tr>
<tr>
<td>2543867</td>
<td>UI: If either the peer_hostname or the peer_asn is invalid, the full screen BGP Service card does not provide the ability to open cards for a selected BGP session.</td>
<td>2.3.0-2.4.1, 3.0.0-3.3.1</td>
<td></td>
</tr>
</table>
<table name="Fixed issues in 3.3.0">
<tr>
<th> Issue ID </th>
<th> Description </th>
<th> Affects </th>
</tr>
<tr>
<td>2556006</td>
<td>NetQ Infra: Customers with cloud deployments who wish to use the lifecycle management (LCM) feature in NetQ 3.3.0 must upgrade their NetQ Cloud Appliance or Virtual Machine as well as the NetQ Agent.</td>
<td>3.2.1</td>
</tr>
<tr>
<td>2549246</td>
<td>UI: Snapshot comparison cards may not render correctly after navigating away from a workbench and then returning to it. If you are viewing the Snapshot comparison card(s) on a custom workbench, refresh the page to reload the data. If you are viewing it on the Cumulus Default workbench, after refreshing the page you must recreate the comparison(s).</td>
<td>2.4.0-3.2.1</td>
</tr>
</table>
</tables>