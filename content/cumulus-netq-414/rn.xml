<tables>
<table name="Open Issues in 4.14.0">
<tr>
<th> Issue ID </th>
<th> Description </th>
<th> Affects </th>
<th> Fixed </th>
</tr>
<tr>
<td>4466349</td>
<td>When you upgrade an HA cluster deployment from a version that is not part of the supported upgrade path, the upgrade might fail and the UI might not load due to expired control plane certificates on the worker nodes.To check whether the certificates have expired, run &lt;code&gt;sudo su&lt;/code&gt; followed by &lt;code&gt;kubeadm certs check-expiration&lt;/code&gt;. If the output displays a date in the past, your certificates are expired. To update the certificates, run &lt;code&gt;kubeadm certs renew all&lt;/code&gt; on each worker node in the cluster. Next, restart the &lt;a href="https://kubernetes.io/docs/concepts/overview/components/#control-plane-components/"&gt;control plane components&lt;/a&gt; with &lt;code&gt;crictl stop CONTAINER_ID&lt;/code&gt;, followed by &lt;code&gt;systemctl restart kubelet&lt;/code&gt;.</td>
<td>4.8.0-4.14.0</td>
<td></td>
</tr>
<tr>
<td>4402969</td>
<td>When you upgrade a cluster deployment from NetQ 4.13 to 4.14 using Base Command Manager (BCM), the operation might appear to fail with the error message {{Warning: Ping to admin app failed. 10.141.0.1 Traceback (most recent call last): File “/usr/bin/netq”, line 404, in &lt;module&gt; rx_reply(sock, sys.argv) File “/usr/bin/netq”, line 126, in rx_reply rx_data = sock.recv(4096) BlockingIOError: [Errno 11] Resource temporarily unavailable}}. You can ignore this message and wait for the upgrade to complete. To verify the actual NetQ version and upgrade status, run the {{netq show status}} command.


</td>
<td>4.14.0</td>
<td></td>
</tr>
<tr>
<td>4328176</td>
<td>When you install or upgrade the {{netq-agents}} and {{netq-apps}} packages, the operation may fail with {{Failed to allocate directory watch: Too many open files}}. To work around this issue, temporarily increase the number of user instances by running {{sudo sysctl -w fs.inotify.max_user_instances=256}}. After you test the temporary fix, you can permanently increase the limit by editing the {{/etc/sysctl.conf}} file and adding the line {{fs.inotify.max_user_instances=256}}.

</td>
<td>4.14.0</td>
<td></td>
</tr>
<tr>
<td>4310939</td>
<td>When a switch becomes rotten or is connected to a different NetQ server without decommissioning it first, the link health view dashboard displays outdated counter values. To work around this issue, wait for NetQ to update and display accurate counter values.</td>
<td>4.13.0-4.14.0</td>
<td></td>
</tr>
<tr>
<td>4309191</td>
<td>In cloud deployments, lifecycle management operations such as device discovery or switch decommissioning might time out and ultimately fail. To work around this issue, restart the LCM executor on the OPTA VM with {{lcm_pod=&apos;kubectl get pod | grep -m1 lcm | awk ‘{print $1}’&apos;; kubectl delete pod $lcm_pod}}.</td>
<td>4.13.0-4.14.0</td>
<td></td>
</tr>
<tr>
<td>4298008</td>
<td>When you upgrade from NetQ 4.11 to 4.13, any pre-existing validation data will be lost.</td>
<td>4.13.0-4.14.0</td>
<td></td>
</tr>
<tr>
<td>4261327</td>
<td>In a 5-node scale deployment, queue histogram data might take up to 5 minutes to load in the UI.</td>
<td>4.13.0-4.14.0</td>
<td></td>
</tr>
<tr>
<td>4248942</td>
<td>When you assign a role to a switch, NetQ might take up to five minutes to reflect the new or updated role in the queue histogram fabric overview page.</td>
<td>4.13.0-4.14.0</td>
<td></td>
</tr>
<tr>
<td>4131550</td>
<td>When you run a topology validation, the full-screen topology validation view might not display the latest results. To work around this issue, refresh the page.</td>
<td>4.12.0-4.14.0</td>
<td></td>
</tr>
<tr>
<td>4126632</td>
<td>In scale deployments with 1,000 or more switches, BGP validations might take up to five minutes to display results in the UI or CLI.</td>
<td>4.13.0-4.14.0</td>
<td></td>
</tr>
<tr>
<td>4100882</td>
<td>When you attempt to export a file that is larger than 200MB, your browser might crash or otherwise prevent you from exporting the file. To work around this issue, use filters in the UI to decrease the size of the dataset that you intend to export.</td>
<td>4.12.0-4.14.0</td>
<td></td>
</tr>
<tr>
<td>3985598</td>
<td>When you configure multiple threshold-crossing events for the same TCA event ID on the same device, NetQ will only display one TCA event for each hostname per TCA event ID, even if both thresholds are crossed or status events are triggered.  </td>
<td>4.11.0-4.14.0</td>
<td></td>
</tr>
<tr>
<td>3800434</td>
<td>When you upgrade NetQ from a version prior to 4.9.0, What Just Happened data that was collected before the upgrade is no longer present.</td>
<td>4.9.0-4.14.0</td>
<td></td>
</tr>
<tr>
<td>3772274</td>
<td>After you upgrade NetQ, data from snapshots taken prior to the NetQ upgrade will contain unreliable data and should not be compared to any snapshots taken after the upgrade. In cluster deployments, snapshots from prior NetQ versions will not be visible in the UI.</td>
<td>4.9.0-4.14.0</td>
<td></td>
</tr>
<tr>
<td>3613811</td>
<td>LCM operations using in-band management are unsupported on switches that use eth0 connected to an out-of-band network. To work around this issue, configure NetQ to use out-of-band management in the {{mgmt}} VRF on Cumulus Linux switches when interface eth0 is in use.</td>
<td>4.8.0-4.14.0</td>
<td></td>
</tr>
</table>
<table name="Fixed Issues in 4.14.0">
<tr>
<th> Issue ID </th>
<th> Description </th>
<th> Affects </th>
</tr>
<tr>
<td>4371014</td>
<td>In the full-screen switch card, the interface charts display incorrect values for transmit (Tx) and receive (Rx) byte rates. The actual values are slightly higher than the displayed values.</td>
<td>4.12.0-4.13.0</td>
</tr>
<tr>
<td>4360421</td>
<td>When you back up your data from a prior NetQ release and restore it after installing NetQ 4.13.0, any switches that were in a rotten state are missing from the NetQ inventory after the upgrade. To work around this issue, decommission any rotten switches before you upgrade and reconnect the agents after the upgrade is complete.</td>
<td>4.13.0</td>
</tr>
<tr>
<td>4360420</td>
<td>When you upgrade to 4.13, network snapshots taken prior to upgrading are not restored.</td>
<td>4.13.0</td>
</tr>
<tr>
<td>4280023</td>
<td>After backing up and restoring your NetQ data, any modifications to default suppression rules will be lost.</td>
<td>4.12.0-4.13.0</td>
</tr>
<tr>
<td>4261089</td>
<td>When you upgrade a cloud deployment, some switches might not appear in the search field or list of hostnames. To work around this issue, decommission the switches, then restart the agent on each switch with the &lt;code&gt; sudo netq config restart agent &lt;/code&gt; command.</td>
<td>4.13.0</td>
</tr>
<tr>
<td>4236491</td>
<td>When you click within a comparison view chart in link health view, the link utilization values in the side menu might differ from the values displayed in the comparison view chart. The values in the comparison chart are aggregated ever hour, whereas the values in the side menu reflect the most recent data. </td>
<td>4.13.0</td>
</tr>
<tr>
<td>3769936</td>
<td>When there is a NetQ interface validation failure for admin state mismatch, the validation failure might clear unexpectedly while one side of the link is still administratively down.</td>
<td>4.9.0-4.13.0</td>
</tr>
</table>
</tables>