<tables>
<table name="Open Issues in 4.5.0">
<tr>
<th> Issue ID </th>
<th> Description </th>
<th> Affects </th>
<th> Fixed </th>
</tr>
<tr>
<td>3491935</td>
<td>NetQ might generate continuous TCA events for the NetQ VM squashfs mounts when disk utilization TCA rules are configured for all hosts.  </td>
<td>4.5.0-4.6.0</td>
<td>4.7.0-4.8.0</td>
</tr>
<tr>
<td>3454057</td>
<td>When you configure more than one TCA rule referencing the same TCA event type, adding additional TCA rules fails with the following message:

Failed to add/update TCA http status_code: 409</td>
<td>4.5.0-4.6.0</td>
<td>4.7.0-4.8.0</td>
</tr>
<tr>
<td>3448057</td>
<td>NetQ NTP validations will report time syncronization failures for switches running the NTP service in the default VRF.</td>
<td>4.5.0-4.6.0</td>
<td>4.7.0-4.8.0</td>
</tr>
<tr>
<td>3446351</td>
<td>When you perform an apt upgrade from NetQ 4.5.0 to version 4.6.0, the {{sudo apt upgrade}} command fails with the following message: 


Setting up shim-signed (1.40.9+15.7-0ubuntu1) ...

mount: /var/lib/grub/esp: special device /dev/vda15 does not exist.

dpkg: error processing package shim-signed (--configure):

installed shim-signed package post-installation script subprocess returned error exit status 32

Errors were encountered while processing:

shim-signed

E: Sub-process /usr/bin/dpkg returned an error code (1)


To work around this issue, run the {{sudo apt remove -y shim-signed grub-efi-amd64-bin --allow-remove-essential}} command and rerun the {{sudo apt upgrade}} command.</td>
<td>4.5.0-4.6.0</td>
<td>4.7.0-4.8.0</td>
</tr>
<tr>
<td>3442456</td>
<td>When an event notification is resolved or acknowledged, the NetQ UI might display a duplicate event with the original notification content and timestamp.</td>
<td>4.2.0-4.6.0</td>
<td>4.7.0-4.8.0</td>
</tr>
<tr>
<td>3438973</td>
<td>When you install NetQ onto your VM, the installation fails with the following messages:


05:57:33.023618: master-node-installer: Installed Debian ...	[ FAILED ]
--------------------------------------
ERROR: Failed to install the master node


This is due to an expired key in the installation tarball. For assistance working around this issue, contact NVIDIA support.</td>
<td>4.3.0-4.5.0</td>
<td>4.6.0-4.8.0</td>
</tr>
<tr>
<td>3431386</td>
<td>When you upgrade your NetQ VM from NetQ 4.5.0 to 4.6.0 using the {{netq upgrade bundle}} command, certain pods are not correctly retagged. To work around this issue, retag and restart the affected pods with the following commands for your deployment after upgrading:

On-premises VMs:

sudo docker tag localhost:5000/fluend-aggregator-opta:1.14.3 docker-registry:5000/fluend-aggregator-opta:1.14.3
sudo docker push docker-registry:5000/fluend-aggregator-opta:1.14.3
sudo kubectl get pods -n default|grep -i fluend-aggregator-opta|awk '{print $1}'|xargs kubectl delete pod -n default

sudo docker tag localhost:5000/cp-schema-registry:7.2.0 docker-registry:5000/cp-schema-registry:7.2.0
sudo docker push docker-registry:5000/cp-schema-registry:7.2.0
sudo kubectl get pods -n default|grep -i cp-schema-registry|awk '{print $1}'|xargs kubectl delete pod -n default

sudo docker tag localhost:5000/cp-kafka:7.2.0 docker-registry:5000/cp-kafka:7.2.0
sudo docker push docker-registry:5000/cp-kafka:7.2.0
sudo kubectl get pods -n default|grep -i kafka-broker|awk '{print $1}'|xargs kubectl delete pod -n default


Cloud VMs:

sudo docker tag localhost:5000/fluend-aggregator-opta:1.14.3 docker-registry:5000/fluend-aggregator-opta:1.14.3
sudo docker push docker-registry:5000/fluend-aggregator-opta:1.14.3
sudo kubectl get pods -n default|grep -i fluend-aggregator-opta|awk '{print $1}'|xargs kubectl delete pod -n default

</td>
<td>4.5.0-4.6.0</td>
<td>4.7.0-4.8.0</td>
</tr>
<tr>
<td>3395385</td>
<td>When you use NetQ LCM to upgrade a Cumulus Linux switch in an MLAG pair, the upgrade might fail.</td>
<td>4.4.1-4.5.0</td>
<td>4.6.0-4.8.0</td>
</tr>
<tr>
<td>3367267</td>
<td>When you upgrade a switch with NetQ LCM using the {{root}} user, the upgrade fails with the following message:  Destination /home/root does not exist.  To work around this issue, perform the upgrade using a different user account.</td>
<td>4.5.0</td>
<td>4.6.0-4.8.0</td>
</tr>
<tr>
<td>3362224</td>
<td>When you configure a new access profile with SSH authentication using the CLI, the command fails with the following log message:


Expecting value: line 1 column 1 (char 0) 


To work around this issue, use the NetQ UI to configure the access profile.</td>
<td>4.5.0</td>
<td>4.6.0-4.8.0</td>
</tr>
<tr>
<td>3360627</td>
<td>When the switch RoCE egress pool buffer limit is configured as unlimited, the maximum buffer usage for RoCE counters might display incorrect values in the NetQ UI.</td>
<td>4.4.1-4.5.0</td>
<td>4.6.0-4.8.0</td>
</tr>
<tr>
<td>2885312</td>
<td>EVPN Validation Type 2 checks might show false Duplicate MAC events for MAC addresses that are not duplicated. An example of this is shown below:

   EVPN Type 2 Test details:
   Hostname          Peer Name         Peer Hostname     Reason                                        Last Changed
   ----------------- ----------------- ----------------- --------------------------------------------- -------------------------
   torc-11           -                 -                 Duplicate Mac 00:02:00:00:00:55 VLAN 1249 at  Sun Dec  5 18:26:14 2021
                                                         torc-21:vx-282 and torc-11:peerlink-3
   </td>
<td>4.1.0-4.8.0</td>
<td></td>
</tr>
</table>
<table name="Fixed Issues in 4.5.0">
<tr>
<th> Issue ID </th>
<th> Description </th>
<th> Affects </th>
</tr>
<tr>
<td>3305144</td>
<td>When you perform a {{netq trace}} between two hosts, the following message might be printed in the output even when the trace is successful:


argument of type ‘NoneType’ is not iterable
</td>
<td>4.4.0-4.4.1</td>
</tr>
<tr>
<td>3303284</td>
<td>When you run the  {{netq show opta-health}} command, it might fail and produce the following error:


ERROR: Expecting value: line 1 column 1 (char 0)
</td>
<td>4.3.0-4.4.1</td>
</tr>
<tr>
<td>3290068</td>
<td>When you back up NetQ data with the {{backuprestore.sh}} script, the operation fails with the following log messages:

Failed to clear all earlier snapshot for keyspace:master. Exiting!
command terminated with exit code 1
Failed to execute /opt/backuprestore/createbackup.sh script on cassandra pod.
Failed to proceed ahead with backup procedure. Exiting !

Contact NVIDIA support for assistance performing a backup.</td>
<td>4.4.0-4.4.1</td>
</tr>
<tr>
<td>3266922</td>
<td>When a NetQ agent sends your NetQ server or OPTA an unexpectedly large number for switch interface counters, {{netq check}} and {{netq show}} commands might fail with the following message:


local variable ‘url’ referenced before assignment
</td>
<td>4.4.0-4.4.1</td>
</tr>
<tr>
<td>3241664</td>
<td>When you start the {{netq-agent}} service, the WJH service is enabled by default.  However, when you run the {{netq config show agent wjh}} command, the output might reflect the WJH service as disabled.</td>
<td>4.4.0-4.4.1</td>
</tr>
<tr>
<td>3231404</td>
<td>When you attempt to reinstall NetQ on a server with an existing NetQ installation using the {{netq install [opta]}} command, the installation fails with the following messages:


master-node-installer: Plugged in release bundle ...    [ FAILED ]
--------------------------------------
ERROR: Failed to install the master node


Command '['/usr/bin/dpkg', '-i', '']' returned non-zero exit status 2.


To work around this issue, run the {{netq bootstrap reset}} command before attempting to reinstall NetQ on your existing server.</td>
<td></td>
</tr>
<tr>
<td>3179145</td>
<td>The NetQ agent does not collect VLAN information from WJH data. This has been resolved, however when you upgrade to a NetQ version with the fix, historical WJH data will not be displayed in the UI.</td>
<td>4.3.0-4.4.1</td>
</tr>
<tr>
<td>3015875</td>
<td>NetQ trace might report incomplete route information when there are multiple default routes in a VRF in the path between the source and destination.</td>
<td>4.1.0-4.4.1</td>
</tr>
</table>
</tables>