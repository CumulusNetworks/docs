<tables>
<table name="Open Issues in 4.8.0">
<tr>
<th> Issue ID </th>
<th> Description </th>
<th> Affects </th>
<th> Fixed </th>
</tr>
<tr>
<td>3819364</td>
<td>When you attempt to delete a scheduled trace using the NetQ UI, the trace record is not deleted.</td>
<td>4.7.0-4.9.0</td>
<td>4.10.0</td>
</tr>
<tr>
<td>3782784</td>
<td>After performing a new NetQ cluster installation, some MLAG and EVPN NetQ validations might incorrectly report errors. To work around this issue, run the {{netq check mlag legacy}} and {{netq check evpn legacy}} commands instead of running a default streaming check. </td>
<td>4.8.0</td>
<td>4.9.0-4.10.0</td>
</tr>
<tr>
<td>3781503</td>
<td>When you upgrade a Cumulus Linux switch running the nslcd service with NetQ LCM, the {{nslcd}} service fails to start after the upgrade. To work around this issue, manually back up your {{nslcd}} configuration and restore it after the upgrade.</td>
<td>4.8.0</td>
<td>4.9.0-4.10.0</td>
</tr>
<tr>
<td>3761602</td>
<td> NetQ does not display queue histogram data for switches running Cumulus Linux 5.8.0 and NetQ agent version 4.8.0. To work around this issue, upgrade the NetQ agent package to 4.9.0.</td>
<td>4.8.0</td>
<td>4.9.0-4.10.0</td>
</tr>
<tr>
<td>3739222</td>
<td>The {{opta-check}} command does not properly validate if the required 16 CPU cores are present on the system for NetQ. The command only presents an error if there are fewer than 8 CPU cores detected.</td>
<td>4.2.0-4.8.0</td>
<td>4.9.0-4.10.0</td>
</tr>
<tr>
<td>3738840</td>
<td>When you upgrade a Cumulus Linux switch configured for TACACS authentication using NetQ LCM, the switch's TACACS configuration is not restored after upgrade.</td>
<td>4.8.0-4.9.0</td>
<td>4.10.0</td>
</tr>
<tr>
<td>3688985</td>
<td>After upgrading a NetQ VM with LDAP authentication configured, adding a new LDAP user to NetQ fails with the error message "LDAP not enabled."</td>
<td>4.8.0</td>
<td>4.9.0-4.10.0</td>
</tr>
<tr>
<td>3676723</td>
<td>When you use the NetQ agent on a Cumulus Linux switch to export gNMI data and there is a period of inactivity in the gNMI stream, the NetQ agent service might stop. To recover from this issue, restart the service with the {{netq config restart agent}} command.</td>
<td>4.7.0-4.8.0</td>
<td>4.9.0-4.10.0</td>
</tr>
<tr>
<td>3670180</td>
<td>The medium Validation Summary card might incorrectly display a failure or lack of data for the latest time interval. To work around this issue, expand the card to the largest view for an accurate representation of validation results.</td>
<td>4.8.0</td>
<td>4.9.0-4.10.0</td>
</tr>
<tr>
<td>3650422</td>
<td>The OPTA-on-switch service does not send agent data when the NetQ CLI is not configured. To work around this issue, configure the NetQ CLI on the switch.</td>
<td>4.8.0</td>
<td>4.9.0-4.10.0</td>
</tr>
<tr>
<td>3644644</td>
<td>When you perform an LCM upgrade of Cumulus Linux on a switch using the {{netq lcm upgrade cl-image}} CLI command, an error message of {{NetQ cloud token invalid}} is displayed though the upgrade completes successfully. This issue is not encountered when using the NetQ LCM UI to perform the upgrade.</td>
<td>4.8.0</td>
<td>4.9.0-4.10.0</td>
</tr>
<tr>
<td>3634648</td>
<td>The topology graph might show unexpected connections when devices in the topology do not have LLDP adjacencies.</td>
<td>4.8.0</td>
<td>4.9.0-4.10.0</td>
</tr>
<tr>
<td>3632378</td>
<td>After you upgrade your on-premises NetQ VM from version 4.7.0 to 4.8.0, NIC telemetry using the Prometheus adapter is not collected. To work around this issue, run the following commands on your NetQ VM:

sudo kubectl set image deployment/netq-prom-adapter netq-prom-adapter=docker-registry:5000/netq-prom-adapter:4.8.0
sudo kubectl set image deployment/netq-prom-adapter prometheus=docker-registry:5000/prometheus-v2.41.0:4.8.0</td>
<td>4.8.0</td>
<td>4.9.0-4.10.0</td>
</tr>
<tr>
<td>3613811</td>
<td>LCM operations using in-band management are unsupported on switches that use eth0 connected to an out-of-band network. To work around this issue, configure NetQ to use out-of-band management in the {{mgmt}} VRF on Cumulus Linux switches when interface eth0 is in use.</td>
<td>4.8.0-4.10.0</td>
<td></td>
</tr>
<tr>
<td>3549877</td>
<td>NetQ cloud deployments might unexpectedly display validation results for checks that did not run on any nodes.</td>
<td>4.6.0-4.8.0</td>
<td>4.9.0-4.10.0</td>
</tr>
<tr>
<td>3429528</td>
<td>EVPN and RoCE validation cards in the NetQ UI might not display data when Cumulus Linux switches are configured with high VNI scale.</td>
<td>4.6.0-4.8.0</td>
<td>4.9.0-4.10.0</td>
</tr>
</table>
<table name="Fixed Issues in 4.8.0">
<tr>
<th> Issue ID </th>
<th> Description </th>
<th> Affects </th>
</tr>
<tr>
<td>3575935</td>
<td>When you upgrade to NetQ 4.7.0, configured premises names might get reset to the default name {{OPID0}}.</td>
<td>4.7.0</td>
</tr>
<tr>
<td>3575934</td>
<td>When you upgrade to NetQ 4.7.0, the password for the {{admin}} user is reset to the default password.</td>
<td>4.7.0</td>
</tr>
<tr>
<td>3555031</td>
<td>NetQ incorrectly reports a low health SSD event on SN5600 switches. To work around this issue, configure an event suppression rule for {{ssdutil}} messages from SN5600 switches in your network.</td>
<td>4.7.0</td>
</tr>
<tr>
<td>3530739</td>
<td>Queue histogram data received from switches might encounter a delay before appearing in the NetQ UI.</td>
<td>4.7.0</td>
</tr>
</table>
</tables>