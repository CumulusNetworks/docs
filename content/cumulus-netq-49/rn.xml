<tables>
<table name="Open Issues in 4.9.0">
<tr>
<th> Issue ID </th>
<th> Description </th>
<th> Affects </th>
<th> Fixed </th>
</tr>
<tr>
<td>3819364</td>
<td>When you attempt to delete a scheduled trace using the NetQ UI, the trace record is not deleted.</td>
<td>4.7.0-4.9.0</td>
<td></td>
</tr>
<tr>
<td>3814701</td>
<td>After you upgrade NetQ, devices that were in a rotten state before the upgrade might not appear in the UI or CLI after the upgrade. To work around this issue, decommission rotten devices before performing the upgrade.</td>
<td>4.9.0</td>
<td></td>
</tr>
<tr>
<td>3813819</td>
<td>When you perform a switch discovery by specifying an IP range, an error message is displayed if switches included in the range have different credentials. To work around this issue, batch switches based on their credentials and run a switch discovery for each batch.</td>
<td>4.9.0</td>
<td></td>
</tr>
<tr>
<td>3813078</td>
<td>When you perform a NetQ upgrade, the upgrade might fail with the following error message:
Command '['kubectl', 'version --client']' returned non-zero exit status 1.
To work around this issue, run the {{netq bootstrap reset keep-db}} command and then reinstall NetQ using the {{netq install}} &lt;a href="https://docs.nvidia.com/networking-ethernet-software/cumulus-netq/More-Documents/NetQ-CLI-Reference-Manual/install/"&gt;command for your deployment.&lt;/a&gt;</td>
<td>4.9.0</td>
<td></td>
</tr>
<tr>
<td>3808200</td>
<td>When you perform a {{netq bootstrap reset}} on a NetQ cluster VM and perform a fresh install with the {{netq install}} command, the install might fail with the following error: 
 master-node-installer: Running sanity check on cluster_vip: 10.10.10.10 Virtual IP 10.10.10.10 is already used
To work around this issue, run the {{netq install}} command again.</td>
<td>4.9.0</td>
<td></td>
</tr>
<tr>
<td>3773879</td>
<td>When you upgrade a switch running Cumulus Linux using NetQ LCM, any configuration files in {{/etc/cumulus/switchd.d}} for adaptive routing or other features are not restored after the upgrade. To work around this issue, manually back up these files and 
restore them after the upgrade.</td>
<td>4.9.0</td>
<td></td>
</tr>
<tr>
<td>3772274</td>
<td>After you upgrade NetQ, data from snapshots taken prior to the NetQ upgrade will contain unreliable data and should not be compared to any snapshots taken after the upgrade. In cluster deployments, snapshots from prior NetQ versions will not be visible in the UI.</td>
<td>4.9.0</td>
<td></td>
</tr>
<tr>
<td>3771124</td>
<td>When you reconfigure a VNI to map to a different VRF or remove and recreate a VNI in the same VRF, NetQ EVPN validations might incorrectly indicate a failure for the VRF consistency test.</td>
<td>4.9.0</td>
<td></td>
</tr>
<tr>
<td>3769936</td>
<td>When there is a NetQ interface validation failure for admin state mismatch, the validation failure might clear unexpectedly while one side of the link is still administratively down.</td>
<td>4.9.0</td>
<td></td>
</tr>
<tr>
<td>3764718</td>
<td>When you reboot the master node of a NetQ cluster deployment, NIC telemetry is no longer collected. To recover from this issue, restart the Prometheus pod with the following commands: 
1. Retrieve the Prometheus pod name with the {{kubectl get pods | grep netq-prom}} command 
2. Restart the pod by deleting the pod name with the {{kubectl delete pod &lt;pod-name&gt;}} command 

Example: 
cumulus@netq-server:~$ kubectl get pods | grep netq-prom
netq-prom-adapter-ffd9b874d-hxhbz                    2/2     Running   0          3h50m
cumulus@netq-server:~$ kubectl delete pod netq-prom-adapter-ffd9b874d-hxhbz </td>
<td>4.9.0</td>
<td></td>
</tr>
<tr>
<td>3760442</td>
<td>When you export events from NetQ to a CSV file, the timestamp of the exported events does not match the timestamp reported in the NetQ UI based on the user profile's time zone setting.</td>
<td>4.9.0</td>
<td></td>
</tr>
<tr>
<td>3752422</td>
<td>When you run a NetQ trace and specify MAC addresses for the source and destination, NetQ displays the message “No valid path to destination” and does not display trace data.</td>
<td>4.9.0</td>
<td></td>
</tr>
<tr>
<td>3738840</td>
<td>When you upgrade a Cumulus Linux switch configured for TACACS authentication using NetQ LCM, the switch's TACACS configuration is not restored after upgrade.</td>
<td>4.8.0-4.9.0</td>
<td></td>
</tr>
<tr>
<td>3721754</td>
<td>After you decommission a switch, the switch's interfaces are still displayed in the NetQ UI in the Interfaces view.</td>
<td>4.9.0</td>
<td></td>
</tr>
<tr>
<td>3688985</td>
<td>After upgrading a NetQ VM with LDAP authentication configured, adding a new LDAP user to NetQ fails with the error message "LDAP not enabled."</td>
<td>4.8.0-4.9.0</td>
<td></td>
</tr>
<tr>
<td>3656965</td>
<td>After you upgrade NetQ and try to decommission a switch, the decommission might fail with the message "Timeout encountered while processing."</td>
<td>4.8.0-4.9.0</td>
<td></td>
</tr>
<tr>
<td>3633458</td>
<td>The legacy topology diagram might categorize devices into tiers incorrectly. To work around this issue, use the updated topology diagram by selecting Topology Beta in the latest version of the NetQ UI.</td>
<td>4.7.0-4.9.0</td>
<td></td>
</tr>
<tr>
<td>3613811</td>
<td>LCM operations using in-band management are unsupported on switches that use eth0 connected to an out-of-band network. To work around this issue, configure NetQ to use out-of-band management in the {{mgmt}} VRF on Cumulus Linux switches when interface eth0 is in use.</td>
<td>4.8.0-4.9.0</td>
<td></td>
</tr>
<tr>
<td>2885312</td>
<td>EVPN Validation Type 2 checks might show false Duplicate MAC events for MAC addresses that are not duplicated. An example of this is shown below:

   EVPN Type 2 Test details:
   Hostname          Peer Name         Peer Hostname     Reason                                        Last Changed
   ----------------- ----------------- ----------------- --------------------------------------------- -------------------------
   torc-11           -                 -                 Duplicate Mac 00:02:00:00:00:55 VLAN 1249 at  Sun Dec  5 18:26:14 2021
                                                         torc-21:vx-282 and torc-11:peerlink-3
   </td>
<td>4.1.0-4.9.0</td>
<td></td>
</tr>
</table>
<table name="Fixed Issues in 4.9.0">
<tr>
<th> Issue ID </th>
<th> Description </th>
<th> Affects </th>
</tr>
<tr>
<td>3782784</td>
<td>After performing a new NetQ cluster installation, some MLAG and EVPN NetQ validations might incorrectly report errors. To work around this issue, run the {{netq check mlag legacy}} and {{netq check evpn legacy}} commands instead of running a default streaming check. </td>
<td>4.8.0</td>
</tr>
<tr>
<td>3781503</td>
<td>When you upgrade a Cumulus Linux switch running the nslcd service with NetQ LCM, the {{nslcd}} service fails to start after the upgrade. To work around this issue, manually back up your {{nslcd}} configuration and restore it after the upgrade.</td>
<td>4.8.0</td>
</tr>
<tr>
<td>3761602</td>
<td> NetQ does not display queue histogram data for switches running Cumulus Linux 5.8.0 and NetQ agent version 4.8.0. To work around this issue, upgrade the NetQ agent package to 4.9.0.</td>
<td>4.8.0</td>
</tr>
<tr>
<td>3739222</td>
<td>The {{opta-check}} command does not properly validate if the required 16 CPU cores are present on the system for NetQ. The command only presents an error if there are fewer than 8 CPU cores detected.</td>
<td>4.2.0-4.8.0</td>
</tr>
<tr>
<td>3676723</td>
<td>When you use the NetQ agent on a Cumulus Linux switch to export gNMI data and there is a period of inactivity in the gNMI stream, the NetQ agent service might stop. To recover from this issue, restart the service with the {{netq config restart agent}} command.</td>
<td>4.7.0-4.8.0</td>
</tr>
<tr>
<td>3670180</td>
<td>The medium Validation Summary card might incorrectly display a failure or lack of data for the latest time interval. To work around this issue, expand the card to the largest view for an accurate representation of validation results.</td>
<td>4.8.0</td>
</tr>
<tr>
<td>3650422</td>
<td>The OPTA-on-switch service does not send agent data when the NetQ CLI is not configured. To work around this issue, configure the NetQ CLI on the switch.</td>
<td>4.8.0</td>
</tr>
<tr>
<td>3644644</td>
<td>When you perform an LCM upgrade of Cumulus Linux on a switch using the {{netq lcm upgrade cl-image}} CLI command, an error message of {{NetQ cloud token invalid}} is displayed though the upgrade completes successfully. This issue is not encountered when using the NetQ LCM UI to perform the upgrade.</td>
<td>4.8.0</td>
</tr>
<tr>
<td>3634648</td>
<td>The topology graph might show unexpected connections when devices in the topology do not have LLDP adjacencies.</td>
<td>4.8.0</td>
</tr>
<tr>
<td>3632378</td>
<td>After you upgrade your on-premises NetQ VM from version 4.7.0 to 4.8.0, NIC telemetry using the Prometheus adapter is not collected. To work around this issue, run the following commands on your NetQ VM:

sudo kubectl set image deployment/netq-prom-adapter netq-prom-adapter=docker-registry:5000/netq-prom-adapter:4.8.0
sudo kubectl set image deployment/netq-prom-adapter prometheus=docker-registry:5000/prometheus-v2.41.0:4.8.0</td>
<td>4.8.0</td>
</tr>
<tr>
<td>3549877</td>
<td>NetQ cloud deployments might unexpectedly display validation results for checks that did not run on any nodes.</td>
<td>4.6.0-4.8.0</td>
</tr>
<tr>
<td>3429528</td>
<td>EVPN and RoCE validation cards in the NetQ UI might not display data when Cumulus Linux switches are configured with high VNI scale.</td>
<td>4.6.0-4.8.0</td>
</tr>
</table>
</tables>